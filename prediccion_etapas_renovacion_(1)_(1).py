# -*- coding: utf-8 -*-
"""Prediccion_Etapas_Renovacion_(1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K2JGIuZZL9-WjucNYNWRPARyzd65Nz9o

# Proyecto Final: Predicción de Oportunidades de Renovación Exitosas
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Objetivo

- Identificar de manera precisa las oportunidades de renovación del cliente que alcanzan la etapa de **"Closed Won"**, permitiendo así comprender los factores y estrategias asociadas con el éxito en la retención de clientes.
- Desarrollar un modelo predictivo avanzado que, basado en diversas variables y patrones históricos, pueda anticipar con precisión si una oportunidad de renovación se convertirá en un éxito o no, brindando así a la empresa la capacidad de tomar medidas proactivas para retener clientes y maximizar la rentabilidad.
"""

import sys

print(sys.version)

"""## Set de Datos

El dataset contiene **122,648** registros con información sobre las oportunidades de renovación de los productos que hacen los clientes de Cisco.

Los campos son los siguientes:

**Deal Id**: ID de la oportunidad de renovación (puede incluir varios registros).

**Product**: Producto a renovar.

**GLOBAL_ULTIMATE_ID**: ID global del cliente.

**ATR**: Valor anual del registro a renovar.

**ATR Band**: Banda en la que se encuentra el valor anual del registro a renovar.

**Stage**: Etapa de la renovación.

**Risk Profile**: Nivel de riesgo de la renovación.

**Professional Services Flag**: Flag que indica si la Unidad de Negocio del Cliente tiene contratados Servicios Profesionales de Cisco.

**Premium Services Flag**: Flag que indica si la Unidad de Negocio del Cliente tiene contratado soporte premium con Cisco (Success Tracks).

**SAV Level 3**: Nivel 3 de la estructura comercial que atiende al cliente.

**INDUSTRY**: Industria en la que se encuentra el cliente.

**Lifecycle Stage**: Etapa de la adopción.

**CONTRACT START DATE**: Fecha de inicio del contrato.

**TERM END DATE**: Fecha de expiración del contrato.

**Duration (Months)**: duración del contrato en meses.

**Duration Band**: Banda en la que se encuentra la duración del contrato.

**WEIGHTED RISK PCT**: Riesgo ponderado del cliente.

**CX_MAX_LAST_TELEMETRY_30_FLG**: Flag de telemetría.

**Is Digital Enrolled**: Flag que inidica si el cliente se encuentra digitalmente enrolado.

### Instalación de librerías
"""

!pip install lime
!pip install plotly

"""## Cargar librerías"""

import pandas as pd
import numpy as np
from datetime import datetime as datetime
from datetime import timedelta as timedelta
import scipy.stats as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report

import lime
import lime.lime_tabular

import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt

# Desplegar todas las filas y columnas
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
# Configurar opciones de visualización para que no se trunquen los números
pd.set_option('display.float_format', '{:.2f}'.format)

"""## Cargar los datos"""

data = pd.read_csv('/content/drive/MyDrive/Hackathon CX 2024/oportunidades_renovacion.csv')

data.shape

data.columns

data.GLOBAL_ULTIMATE_ID.nunique()

#Checar si hay valores nulos en los datos
data.isnull().sum()

# Eliminar valores nulos en las columnas especificadas
data = data.dropna(subset=["Deal Id", "Professional Services Flag", "CONTRACT START DATE"])

data.info()

print(len(data.columns)) # Should print 21

"""## Limpieza de los datos"""

# Convertir las columnas de fecha a datetime
data['Contract_Start_Date'] = pd.to_datetime(data['CONTRACT START DATE'], format='%d/%m/%Y')
data['Term_End_Date'] = pd.to_datetime(data['TERM END DATE'], format='%d/%m/%Y')

data.shape

# Eliminar registros donde la fecha de inicio de contrato es mayor que la fecha de término de contrato
data = data.drop(data[data['Contract_Start_Date'] > data['Term_End_Date']].index)

data.shape

# Remover todos los símbolos no deseados de las columnas y convertir a tipo float
data['ATR'] = data['ATR'].replace('[^\d.]', '', regex=True).astype(float)
data['Duration (Months)'] = data['Duration (Months)'].replace(' -   ', 0)
data['Duration (Months)'] = pd.to_numeric(data['Duration (Months)'])

data.shape

# Ordenar los datos primero por CX Customer BU Id, luego por Contract_Start_Date y finalmente por Term_End_Date
data = data.sort_values(by=['GLOBAL_ULTIMATE_ID', 'Contract_Start_Date'], ascending=[True, False])

data.head()

"""## Feature Engineering"""

data.shape

"""### Antigüedad del Cliente"""

# Calculamos la fecha más antigua por cliente
data['Fecha_Mas_Antigua'] = data.groupby('GLOBAL_ULTIMATE_ID')['Contract_Start_Date'].transform('min')

# Calculamos la antigüedad del cliente en días restando la fecha más antigua del contrato de la fecha actual
data['Antigüedad del Cliente (días)'] = (pd.to_datetime('today') - data['Fecha_Mas_Antigua']).dt.days

data.shape

print(len(data.columns)) # Should print 19

data.info()

"""### Conteo de Oportunidades por Cliente"""

# Calcular el conteo único de Deal ID para cada ID de cliente en cada registro
#data['Deal_ID_Count'] = data.groupby('GLOBAL_ULTIMATE_ID')['Deal Id'].transform('nunique')
# Calcula el conteo incremental de Deal_ID único para cada Customer_ID
data['Deal_ID_Count'] = data.groupby('GLOBAL_ULTIMATE_ID')['Deal Id'].cumcount() + 1

data.shape

"""### Días Transcurridos desde el Contrato Anterior"""

# Agrupa los contratos por 'GLOBAL_ULTIMATE_ID' y calcula la diferencia en días entre las fechas de inicio del contrato actual y el contrato anterior del mismo cliente
data['Dias_Entre_Contratos_Pasado_Y_Actual'] = data.groupby('GLOBAL_ULTIMATE_ID')['Contract_Start_Date'].diff().dt.days

# Toma el valor absoluto de la diferencia en días para garantizar que todos los valores sean positivos
data['Dias_Entre_Contratos_Pasado_Y_Actual'] = data['Dias_Entre_Contratos_Pasado_Y_Actual'].abs().fillna(0).astype(int)

"""### Media Móvil de la Duración del Contrato en Meses"""

# Calcula la media móvil agrupada por 'GLOBAL_ULTIMATE_ID' para la columna 'Duration (Months)'
media_movil = data.groupby('GLOBAL_ULTIMATE_ID')['Duration (Months)'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)

# Agrega los resultados como una nueva columna llamada 'media_movil' en el DataFrame original
data['media_movil_duracion'] = media_movil

"""### Media Movil del ATR"""

# Calcula la media móvil agrupada por 'GLOBAL_ULTIMATE_ID' para la columna 'Duration (Months)'
media_movil_atr = data.groupby('GLOBAL_ULTIMATE_ID')['ATR'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)

# Agrega los resultados como una nueva columna llamada 'media_movil' en el DataFrame original
data['media_movil_atr'] = media_movil_atr

data.shape

"""### Día y Mes del Contrato"""

# Extraer información útil de las fechas
data['Start Month'] = data['Contract_Start_Date'].dt.month
data['End Month'] = data['Term_End_Date'].dt.month

data['Start Day'] = data['Contract_Start_Date'].dt.day
data['End Day'] = data['Term_End_Date'].dt.day

data.shape

"""### Día de la Semana"""

data['Start Day of Week'] = data['Contract_Start_Date'].dt.dayofweek
data['End Day of Week'] = data['Term_End_Date'].dt.dayofweek

data.shape

data.info()

"""### Temporada del Año"""

# Estación del año (suponiendo que el año se divide en cuatro estaciones)
def season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Fall'

data['Start Season'] = data['Start Month'].apply(season)
data['End Season'] = data['End Month'].apply(season)

data.shape

# Lista de columnas relevantes para el modelo de customer churn
relevant_columns = ['GLOBAL_ULTIMATE_ID', 'Product', 'ATR', 'ATR Band', 'INDUSTRY', 'WEIGHTED RISK PCT', 'Is Digital Enrolled', 'De-Risk Flag', 'Professional Services Flag', 'Premium Services Flag', 'CSE Flag','Lifecycle Stage',
                    'Duration (Months)', 'Duration Band','Antigüedad del Cliente (días)', 'Dias_Entre_Contratos_Pasado_Y_Actual','media_movil_duracion','media_movil_atr', 'Contract_Start_Date',
                    'Term_End_Date', 'Start Month', 'End Month', 'Start Day','End Day', 'Start Day of Week', 'End Day of Week', 'Start Season', 'End Season','Deal_ID_Count', 'Stage'

]

# Filtrar el DataFrame 'data' para incluir solo las columnas relevantes
data = data[relevant_columns]

data.shape

# Número de filas
n_rows = data.shape[0]
# Generar tres columnas de sentimiento con probabilidad diferente
np.random.seed(42)
sentiment_csat = np.random.choice([1, 0], size=n_rows, p=[0.6, 0.4])  # 60% positivo
tac_sentiment = np.random.choice([1, 0], size=n_rows, p=[0.7, 0.3])  # 70% positivo
maker_sentiment = np.random.choice([1, 0], size=n_rows, p=[0.5, 0.5])  # 50% positivo

# Crear columnas en el DataFrame original
data['SENTIMENT_CSAT'] = sentiment_csat
data['TAC_SENTIMENT'] = tac_sentiment
data['MAKER_SENTIMENT'] = maker_sentiment

"""### Análisis Descriptivo"""

data.describe()

# Filtrar los datos
filtered_data = data[data['Stage'].isin(['6 - Closed Won', '6 - Closed Lost'])]

# Calcular los recuentos
stage_counts = filtered_data['Stage'].value_counts()

# Crear gráfico de barras con Plotly
fig = px.bar(x=stage_counts.index, y=stage_counts.values, color=stage_counts.index)

# Actualizar el diseño
fig.update_layout(
    xaxis_title='Stage',
    yaxis_title='Clientes',
    title='Count of Customer churned or not churned'
)

fig.show()

ax = sns.kdeplot(data["Duration (Months)"][(data["Stage"] == '6 - Closed Won') ],
                color="Gold", shade = True);
ax = sns.kdeplot(data["Duration (Months)"][(data["Stage"] == '6 - Closed Lost') ],
                ax =ax, color="Green", shade= True);
ax.legend(["Closed Won","Closed Lost"],loc='upper right');
ax.set_ylabel('Número de Clientes');
ax.set_xlabel('Duration (Months) ');
ax.set_title('Distribución de Duración del Contrato en meses ');

# Filtering data
lost_data = data["Duration (Months)"][data["Stage"] == '6 - Closed Won']
won_data = data["Duration (Months)"][data["Stage"] == '6 - Closed Lost']

# Creating KDE plots
fig = go.Figure()

fig.add_trace(go.Histogram(x=lost_data, histnorm='probability density', name='Closed Lost', marker_color='Gold', opacity=0.5))
fig.add_trace(go.Histogram(x=won_data, histnorm='probability density', name='Closed Won', marker_color='Green', opacity=0.5))

# Update layout
fig.update_layout(barmode='overlay',
                  xaxis=dict(title='Duration (Months)'),
                  yaxis=dict(title='Density'),
                  title='Distribución de la duración del contrato en meses por Estado de la Renovación',
                  legend=dict(x=0.85, y=0.95),
                  )

fig.show()

data.shape

# Filtrar los datos
filtered_data = data[data['Stage'].isin(['6 - Closed Won', '6 - Closed Lost'])]

# Crear el gráfico de caja con Plotly
fig = px.box(filtered_data, x='Stage', y='Duration (Months)')

# Actualizar propiedades del eje y
fig.update_yaxes(title_text='Duración del Contrato en meses', row=1, col=1)

# Actualizar propiedades del eje x
fig.update_xaxes(title_text='Etapa de la Renovación', row=1, col=1)

# Actualizar tamaño y título
fig.update_layout(autosize=True, width=750, height=600,
    title_font=dict(size=25, family='Courier'),
    title='Duración en meses vs Etapa de la Renovación',
)

fig.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Crear el gráfico de caja interactivo con Plotly
fig = px.box(datos_filtrados, x='Stage', y='WEIGHTED RISK PCT',
             title='Distribución del Riesgo por Etapa de la Renovación',
             labels={'Stage': 'Etapa', 'WEIGHTED RISK PCT': 'Porcentaje de Riesgo Ponderado'})
fig.update_layout(xaxis={'categoryorder': 'array', 'categoryarray': etapas_relevantes})
fig.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Graficar la relación entre Stage y Premium Services Flag
plt.figure(figsize=(10, 6))
sns.countplot(data=datos_filtrados, x='Premium Services Flag', hue='Stage')
plt.title('Distribución de Clientes que cuentan con Servicios Premium por Etapa de la Renovación')
plt.xlabel('Cuenta con Servicios Premium')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Etapa de la Renovación')
plt.tight_layout()
plt.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Crear un dataframe para contar
count_df = datos_filtrados.groupby(['Premium Services Flag', 'Stage']).size().reset_index(name='Count')

# Graficar con Plotly
fig = px.bar(count_df, x='Premium Services Flag', y='Count', color='Stage',
             title='Distribución de Clientes que cuentan con Servicios Premium por Etapas de la Renovación',
             labels={'Cuenta con Servicios Premium': 'Premium Services Flag', 'Count': 'Count', 'Stage': 'Stage'},
             barmode='group')

# Rotar las etiquetas del eje x
fig.update_layout(xaxis=dict(tickangle=45))

fig.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Graficar la relación entre Stage y Professional Services Flag
plt.figure(figsize=(10, 6))
sns.countplot(data=datos_filtrados, x='Professional Services Flag', hue='Stage')
plt.title('Distribución de Clientes que cuentas con Servicios Profesionales por Etapa de la Renovación')
plt.xlabel('Cuenta con Servicios Profesionales')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Stage')
plt.tight_layout()
plt.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Crear un dataframe para contar
count_df = datos_filtrados.groupby(['Professional Services Flag', 'Stage']).size().reset_index(name='Count')

# Graficar con Plotly
fig = px.bar(count_df, x='Professional Services Flag', y='Count', color='Stage',
             title='Distribución de Clientes que cuentan con Servicios Profesionales por Etapa de la Renovación',
             labels={'Professional Services Flag': 'Professional Services Flag', 'Count': 'Count', 'Stage': 'Stage'},
             barmode='group')

# Rotar las etiquetas del eje x
fig.update_layout(xaxis=dict(tickangle=45))

fig.show()

# Filtrar los datos para incluir solo las etapas relevantes
etapas_relevantes = ['6 - Closed Won', '6 - Closed Lost']
datos_filtrados = data[data['Stage'].isin(etapas_relevantes)]

# Crear un gráfico de barras para la relación entre Stage y ATR PRODUCT
plt.figure(figsize=(12, 6))
sns.countplot(x='Product', hue='Stage', data=datos_filtrados)
plt.title('Relación entre la Etapa de la Renovación y los Productos adquiridos por el Cliente')
plt.xlabel('Producto')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Etapa de Renovación')
plt.tight_layout()
plt.show()

# Crear un gráfico de barras para la relación entre Stage y ATR PRODUCT
plt.figure(figsize=(12, 6))
order = ['01_Purchase', '02_Onboard', '03_Implement', '04_Use', '05_Engage', '06_Adopt', '07_Optimize', '08_Renew', '999_NA']
sns.countplot(x='Lifecycle Stage', hue='Stage', data=datos_filtrados, order=order)
plt.title('Relación entre la Etapa de la Renovación y el Progreso de la Adopción')
plt.xlabel('Lifecycle Stage')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Etapa de Renovación')
plt.tight_layout()
plt.show()

data.shape

# Filtrar los datos por etapas relevantes
closed_lost = data[data['Stage'] == '6 - Closed Lost']['ATR']
closed_won = data[data['Stage'] == '6 - Closed Won']['ATR']

# Crear las trazas de densidad para Closed Lost y Closed Won
fig = go.Figure()
fig.add_trace(go.Histogram(x=closed_lost, histnorm='probability density',
                            name='Closed Lost', marker=dict(color='Gold'), opacity=0.5))
fig.add_trace(go.Histogram(x=closed_won, histnorm='probability density',
                            name='Closed Won', marker=dict(color='Green'), opacity=0.5))

# Actualizar el diseño del gráfico
fig.update_layout(title='ATR por Etapa de la Renovación',
                  xaxis_title='ATR',
                  yaxis_title='Densidad',
                  barmode='overlay')

# Mostrar el gráfico
fig.show()

# Mapear las categorías de Start Season a números
season_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}
data['Start Season Numerical'] = data['Start Season'].map(season_mapping)

# Trama de densidad de kernel para "Closed Won"
ax = sns.kdeplot(data["Start Season Numerical"][data["Stage"] == '6 - Closed Won'],
                 color="Gold", shade=True)

# Trama de densidad de kernel para "Closed Lost"
ax = sns.kdeplot(data["Start Season Numerical"][data["Stage"] == '6 - Closed Lost'],
                 ax=ax, color="Green", shade=True)

# Agregar leyendas y etiquetas
ax.legend(["Closed Won", "Closed Lost"], loc='upper right')
ax.set_ylabel('Densidad')
ax.set_xlabel('Start Season')
ax.set_title('Distribución de Temporada de Inicio del Contrato')

# Mostrar el gráfico
plt.show()



# Filtrar el DataFrame para incluir solo las dos etapas específicas
filtered_data = data[data['Stage'].isin(['6 - Closed Won', '6 - Closed Lost'])]

# Contar el número de contratos por temporada y etapa
season_stage_counts = filtered_data.groupby(['Start Season', 'Stage']).size().unstack()

# Crear el gráfico de barras apiladas
season_stage_counts.plot(kind='bar', stacked=True, figsize=(10, 6))

# Añadir etiquetas y título
plt.xlabel('Start Season')
plt.ylabel('Número de Contratos')
plt.title('Distribución de Temporada de Inicio del Contrato por Stage')

# Mostrar el gráfico
plt.legend(title='Stage')
plt.show()

data.shape

variables_numericas =  ['ATR',  'WEIGHTED RISK PCT',
'Duration (Months)','Antigüedad del Cliente (días)', 'Dias_Entre_Contratos_Pasado_Y_Actual','media_movil_duracion','media_movil_atr', 'Start Month', 'End Month', 'Start Day','End Day', 'Start Day of Week', 'End Day of Week','Deal_ID_Count']

# Calcular la matriz de correlación
correlation_matrix = data[variables_numericas].corr()

# Crear la matriz de correlación con Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

# Columnas categóricas a convertir en dummies
categorical_cols = ['Product', 'Professional Services Flag', 'Premium Services Flag', 'CSE Flag', 'Lifecycle Stage', 'ATR Band','Is Digital Enrolled', 'De-Risk Flag', 'Duration Band', 'INDUSTRY', 'Start Season', 'End Season']

# Obtener variables dummy para las columnas categóricas
data_dummies = pd.get_dummies(data[categorical_cols], prefix=categorical_cols)

# Concatenar las variables dummy con el DataFrame original
data = pd.concat([data, data_dummies], axis=1)

# Eliminar las columnas originales categóricas si ya no son necesarias
data.drop(categorical_cols, axis=1, inplace=True)

data.shape

data.to_csv('/content/drive/MyDrive/Hackathon CX 2024/oportunidades_renovacion_preprocesadas.csv', index=False)

data.head()

data_entrenamiento = data[~data['Stage'].isin(['0 - Target','6 - Closed Cancelled', '2 - Qualify', '3 - Proposal', '5 - Negotiate', '4 - Selection', '1 - Prospect'])]

data_entrenamiento.set_index("GLOBAL_ULTIMATE_ID", inplace=True)

data_entrenamiento.shape

# Seleccionar las características y la variable objetivo
columnas_a_eliminar = ['Stage', 'Contract_Start_Date', 'Term_End_Date']
X = data_entrenamiento.drop(columnas_a_eliminar, axis=1)
y = data_entrenamiento['Stage']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

X.shape

X.head()

X.info()

X.to_csv('/content/drive/MyDrive/Hackathon CX 2024/X2.csv', index=True)

"""## Creación y Entrenamiento del modelo de clasificación: RandomForest"""

# Entrenar el modelo de Random Forest
model = RandomForestClassifier(n_estimators=150,     # Número de árboles en el bosque
                               max_depth=None,       # Profundidad máxima de cada árbol
                               min_samples_split=2,  # Número mínimo de muestras requeridas para dividir un nodo interno
                               min_samples_leaf=1,   # Número mínimo de muestras requeridas para ser un nodo hoja
                               random_state=42,      # Semilla para reproducibilidad
                               class_weight= 'balanced')


model.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = model.predict(X_test)

# Evaluar la precisión del modelo
accuracy = accuracy_score(y_test, y_pred)
print("Precisión del modelo:", accuracy)

print(classification_report(y_test, y_pred))

#dataframe con las predicciones

y_pred_df = pd.DataFrame(y_pred, columns=['Predicted Stage'])
y_pred_df.head()

y_pred_df.index = X_test.index

#Juntar X_test con predicciones en un solo dataframe

merged_df = pd.concat([X_test, y_pred_df], axis=1)
merged_df.head()

# Calcular la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Crear un DataFrame de la matriz de confusión
labels = sorted(y_test.unique())
confusion_df = pd.DataFrame(cm, index=labels, columns=labels)

# Visualizar la matriz de confusión
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_df, annot=True, fmt='g', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Interpretabilidad del Modelo"""

# Obtener los nombres de las características
feature_names = X_train.columns.tolist()

# Convertir X_train a DataFrame de pandas si no lo es ya
X_train = pd.DataFrame(X_train, columns=feature_names)

# Obtener las etiquetas únicas de las clases
unique_classes = sorted(set(y_train))

# Mapear los números de clase a nombres de clase
class_names = [f'Class {c}' for c in unique_classes]
X_train.reset_index(drop=True, inplace=True)
# Crear una instancia del explainer de LIME
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=feature_names, class_names=class_names, discretize_continuous=True)

# Seleccionar una observación para explicar (puedes cambiar el índice)
observation = X_test.iloc[20]
# Explicar la predicción utilizando LIME
exp = explainer.explain_instance(observation, model.predict_proba)

# Mostrar la explicación
exp.show_in_notebook(show_table=True)

"""## Cálculo de Feature Importance"""

# Feature importance de primeras 15 columnas
importances = model.feature_importances_

# Sort importances and select top 10
sorted_indices = np.argsort(importances)[::-1][:35]
sorted_features = X_train.columns[sorted_indices]

# Create a bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x=importances[sorted_indices], y=sorted_features)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Top 10 Feature Importances', fontsize=14)
plt.show()

"""### Predicciones en Data nueva"""

data_validacion = data[data['Stage'].isin(['2 - Qualify', '3 - Proposal', '5 - Negotiate', '4 - Selection', '1 - Prospect'])]

data_validacion.head()

data_validacion.set_index("GLOBAL_ULTIMATE_ID", inplace=True)

# Seleccionar las características y la variable objetivo
columnas_a_eliminar = ['Stage', 'Contract_Start_Date', 'Term_End_Date']
X = data_validacion.drop(columnas_a_eliminar, axis=1)

# Predecir en el conjunto de prueba
y_valid = model.predict(X)

#dataframe con las predicciones
y_valid_df = pd.DataFrame(y_valid, columns=['Predicted Stage'])
y_valid_df.head()

#Juntar X_test con predicciones en un solo dataframe
y_valid_df.index = X.index
merged_df_validaciones = pd.concat([X, y_valid_df], axis=1)
merged_df_validaciones.head()

merged_df_validaciones[merged_df_validaciones["Predicted Stage"] == "6 - Closed Lost"].head()

merged_df_validaciones[merged_df_validaciones["Predicted Stage"] == "6 - Closed Won"].head()

import joblib

# Ruta en Google Drive
model_path = '/content/drive/MyDrive/Hackathon CX 2024/model.pkl'

# Guardar modelo entrenado
joblib.dump(model, model_path)
print(f'Modelo guardado en: {model_path}')

print(f"Tipo de objeto del modelo cargado: {type(model)}")

!python -c "import sklearn; print(sklearn.__version__)"